\section{Symmetric matrices}

\subsection{Quadratic forms}

A matrix $A$ is symmetric if $A = A^T$.  For each symmetric matrix $A$,
there is an associated quadratic form $x^T A x$.  Even if you forgot
them from our lightning review of linear algebra, you are likely familiar
with quadratic forms from a multivariate calculus class, where they appear
in the context of the second derivative test.  One expands
\[
  F(x+u) = F(x) + F'(x) u + \frac{1}{2} u^T H(x) u + O(\|u\|^3),
\]
and notes that at a stationary point where $F'(x) = 0$, the dominant
term is the quadratic term.  When $H$ is {\em positive definite} or
{\em negative definite}, $x$ is a strong local minimum or maximum,
respectively. When $H$ is indefinite, with both negative and positive
eigenvalues, $x$ is a saddle point.  When $H$ is semi-definite, one has
to take more terms in the Taylor series to determine whether the point
is a local extremum.

If $B$ is a nonsingular matrix, then we can write $x = By$ and $x^T A
x = y^T (B^T A B) y$.  So an ``uphill'' direction for $A$ corresponds
to an ``uphill'' direction for $B^T A B$; and similarly with downhill
directions.  More generally, $A$ and $B^T A B$ have the same {\em inertia},
where the inertia of a symmetric $A$ is the triple
\[
  (\mbox{\# pos eigenvalues},
   \mbox{\# zero eigenvalues},
   \mbox{\# neg eigenvalues}).
\]

Now suppose that $A = LU$, where $L$ is unit lower triangular and $U$
is upper triangular.  If we let $D$ be the diagonal part of $U$, we
can write $A = LDM^T$, where $L$ and $M$ are both unit lower triangular
matrices.  Noting that $A^T = (LDM^T)^T = M D L^T = M (LD)^T$ and
that the $LU$ factorization of a matrix is unique, we find $M = L$
and $LD = DM^T = U$.  Note that $D$ has the same inertia as $A$.

The advantage of the $LDL^T$ factorization over the $LU$ factorization
is that we need only compute and store one triangular factor, and so
$LDL^T$ factorization costs about half the flops and storage of $LU$
factorization.  We have the same stability issues for $LDL^T$
factorization that we have for ordinary $LU$ factorization, so in
general we might compute
\[
  P A P^T = LDL^T,
\]
where the details of various pivoting schemes are described in the book.

\subsection{Positive definite matrices}

A symmetric matrix is positive definite if $x^T A x > 0$ for all
nonzero $x$.  If $A$ is symmetric and positive definite, then $A =
LDL^T$ where $D$ has all positive elements (because $A$ and $D$ have
the same inertia).  Thus, we can write $A = (LD^{1/2})(LD^{1/2})^T =
\hat{L} \hat{L}^T$.  The matrix $\hat{L}$ is a Cholesky factor of $A$.

There are several useful properties of SPD matrices that we will
use from time to time:
\begin{enumerate}
\item
  The inverse of an SPD matrix is SPD.

  {\bf Proof:}
  If $x^T A x > 0$ for all $x \neq 0$, then we cannot have $Ax = 0$
  for nonzero $x$.  So $A$ is necessarily nonsingular.  Moreover,
  \[
    x^T A^{-1} x = (A^{-1} x)^T A (A^{-1} x)
  \]
  must be positive for nonzero $x$ by positive-definiteness of $A$.
  Therefore, $A^{-1}$ is SPD.

\item
  Any minor of an SPD matrix is SPD.

  {\bf Proof:}
  Without loss of generality, let $M = A_{11}$.  Then for any appropriately
  sized $x$,
  \[
    x^T M x = \begin{bmatrix} x \\ 0 \end{bmatrix}^T A \begin{bmatrix} x \\ 0 \end{bmatrix} > 0
  \]
  for $x \neq 0$.  Therefore, $M$ is positive definite.

\item
  Any Schur complement of an SPD matrix is SPD

  {\bf Proof:}
  A Schur complement in $A$ is the inverse of a minor of an inverse of $A$.
  By the two arguments above, this implies that any Schur complement of
  an SPD matrix is SPD.

\item If $M$ is a minor of $A$, $\kappa_2(M) \leq \kappa_2(A)$.

  {\bf Proof:}
  The largest and smallest singular values of an SPD matrix are the
  same as the largest and smallest eigenvalues; they can be written
  as
  \[
    \sigma_1(A) = \max_{\|x\|_2 = 1} x^T A x, \quad
    \sigma_{\min}(A) = \min_{\|x\|_2 = 1} x^T A x.
  \]
  Without loss of generality, let $M = A_{11}$.  Then
  \[
    \sigma_1(M)
    = \max_{\|x\|_2 = 1} x^T M x
    = \max_{\|x\|_2 = 1}
      \begin{bmatrix} x \\ 0 \end{bmatrix}^T A
      \begin{bmatrix} x \\ 0 \end{bmatrix}
    \leq \max_{\|z\|_2 = 1} z^T A z = \sigma_1(A)
  \]
  and similarly $\sigma_{\min}(M) \geq \sigma_{\min}(A)$.
  The condition numbers are therefore
  \[
    \kappa_2(M) =
    \frac{\sigma_1(M)}{\sigma_{\min}(M)} \leq
    \frac{\sigma_1(A)}{\sigma_{\min}(A)} = \kappa_2(A).
  \]

\item If $S$ is a Schur complement in $A$, $\kappa_2(S) \leq \kappa_2(A)$.

  {\bf Proof:}  This is left as an exercise.
\end{enumerate}
