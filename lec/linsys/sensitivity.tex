\section{Error analysis for linear systems}

We now discuss the sensitivity of linear systems to perturbations.
This is relevant for two reasons:
\begin{enumerate}
\item Our standard recipe for getting an error bound for a computed
  solution in the presence of roundoff is to combine a backward
  error analysis (involving only features of the algorithm) with
  a sensitivity analysis (involving only features of the problem).
\item Even without rounding error, it is important to understand
  the sensitivity of a problem to the input variables if the inputs
  are in any way inaccurate (e.g.~because they come from measurements).
\end{enumerate}
We describe several different bounds that are useful in different
contexts.

\subsection{First-order analysis}

We begin with a discussion of the first-order sensitivity analysis
of the system
\[
  Ax = b.
\]
Using our favored variational notation, we have the following relation
between perturbations to $A$ and $b$ and perturbations to $x$:
\[
  \delta A \, x + A \, \delta x = \delta b,
\]
or, assuming $A$ is invertible,
\[
  \delta x = A^{-1} (\delta b - \delta A \, x).
\]
We are interested in relative error, so we divide through by $\|x\|$:
\[
  \frac{\|\delta x\|}{\|x\|} \leq
  \frac{\|A^{-1} \delta b \|}{\|x\|} + \frac{\|A^{-1} \delta A \, x\|}{\|x\|}
\]
The first term is bounded by
\[
  \frac{\|A^{-1} \delta b \|}{\|x\|} \leq
  \frac{\|A^{-1}\| \|\delta b\|}{\|x\|} =
  \kappa(A) \frac{\|\delta b\|}{\|A\| \|x\|} \leq
  \kappa(A) \frac{\|\delta b\|}{\|b\|}
\]
and the second term is bounded by
\[
  \frac{\|A^{-1} \delta A \, x\|}{\|x\|} \leq
  \frac{\|A^{-1} \| \|\delta A\| \|x\|}{\|x\|} =
  \kappa(A) \frac{\|\delta A\|}{\|A\|}
\]
Putting everything together, we have
\[
  \frac{\|\delta x\|}{\|x\|} \leq\kappa(A)
    \left( \frac{\|\delta A\|}{\|A\|} + \frac{\|\delta b\|}{\|b\|} \right),
\]
That is, the relative error in $x$ is (to first order) bounded by the
condition number times the relative errors in $A$ and $b$.

\subsection{Beyond first order}

What if we want to go beyond the first-order error analysis?
Suppose that
\[
  Ax = b \quad \mbox{ and } \quad \hat{A} \hat{x} = \hat{b}.
\]
Then (analogous to our previous manipulations),
\[
  (\hat{A}-A) \hat{x} + A(\hat{x}-x) = \hat{b}-b
\]
from which we have
\[
  \hat{x}-x = A^{-1} \left( (\hat{b}-b) - E \hat{x} \right),
\]
where $E \equiv \hat{A}-A$.  Following the same algebra as before,
we have
\[
  \frac{\|\hat{x}-x\|}{\|x\|} \leq
  \kappa(A) \left(
    \frac{\|E\|}{\|A\|} \frac{\|\hat{x}\|}{\|x\|} +
    \frac{\|\hat{b}-b\|}{\|b\|}
  \right).
\]
Assuming $\|A^{-1}\| \|E\| < 1$, a little additional algebra
(left as an exercise to the student) yields
\[
\frac{\|\hat{x}-x\|}{\|x\|} \leq
\frac{\kappa(A)}{1-\|A^{-1}\|\|E\|} \left(
  \frac{\|E\|}{\|A\|} +
  \frac{\|\hat{b}-b\|}{\|b\|}
\right).
\]

Is this an important improvement on the first order bound?
Perhaps not, for two reasons:
\begin{itemize}
\item One typically cares about the order of magnitude of possible error,
  not the exact bound, and
\item The first-order bound and the ``true'' bound only disagree when
  both are probably pretty bad.  When our house is in flames, our first
  priority is not to gauge whether the garage will catch as well;
  rather, we want to call the firefighters to put it out!
\end{itemize}

\subsection{Componentwise relative bounds}

What if we have more control over the perturbations than a simple bound
on the norms?  For example, we might have a componentwise perturbation
bound
\[
  |\delta A| < \epsilon_A |A| \quad |\delta b| < \epsilon_b |b|,
\]
and neglecting $O(\epsilon^2)$ terms, we obtain
\[
  |\delta x|
  \leq |A^{-1}| \left( \epsilon_b |b| + \epsilon_A |A| |x| \right)
  \leq (\epsilon_b+\epsilon_A) |A^{-1}| |A| |x|.
\]
Taking any vector norm such that $\|\,|x|\,\|=\|x\|$, we have
\[
  \|\delta x\| \leq (\epsilon + \epsilon') \| \, |A^{-1}| \, |A| \, \|.
\]
The quantity
$\kappa_{\mathrm{rel}}(A) = \| \, |A^{-1}| \, |A| \, \|$ is the
componentwise relative condition number (also known as the Skeel
condition number).

\subsection{Residual-based bounds}

The {\em residual} for an approximate solution $\hat{x}$ to the
equation $Ax = b$ is
\[
  r = A \hat{x} - b.
\]
We can express much simpler error bounds in terms of the residual,
using the relation
\[
  \hat{x}-x = A^{-1} r;
\]
taking norms immediately gives
\[
  \|\hat{x}-x\| \leq \|A^{-1}\| \|r\|
\]
and for any vector norm such that $\|\,|x|\,\|=\|x\|$, we have
\[
  \|\hat{x}-x\| \leq \| \, |A^{-1}| |r| \, \|.
\]
Note that we can re-cast a residual error as a backward error on $A$
via the relation
\[
  \left( A - \frac{r\hat{x}^T}{\|\hat{x}\|^2} \right) \hat{x} = b.
\]

\subsection{Shape of error}

So far, we have only really discussed the {\em magnitude} of
errors in a linear solve, but it is worth taking a moment to
consider the {\em shape} of the errors as well.  In particular,
suppose that we want to solve $Ax = b$, and we have the
singular value decomposition
\[
  A = U \Sigma V^T.
\]
If $\sigma_n(A) \ll \sigma_1(A)$, then $\kappa_2 = \sigma_1/\sigma_n \gg q$,
and we expect a large error.  But is this the end of the story?
Suppose that $A$ satisfies
\[
  1 \geq \sigma_1 \geq \ldots \geq \sigma_k \geq C_1 ~~ > ~~
  C_2 \geq \sigma_{k+1} \geq \ldots \geq \sigma_n > 0.
\]
where $C_1 \gg C_2$.  Let $r = A \hat{x} - b$, so that $Ae = r$
where $e = \hat{x}-x$.  Then
\[
  e = A^{-1} r = V \Sigma^{-1} U^T r = V \Sigma^{-1} \tilde{r}
    = \sum_{j=1}^n \frac{\tilde{r}_j}{\sigma_j} v_j.
\]
where $\|\tilde{r}\| = \|U^T r\| = \|r\|$.  Split this as
\[
  e = e_1 + e_2
\]
where we have a controlled piece
\[
  \|e_1\| = \left\| \sum_{j=1}^k \frac{\tilde{r}_j}{\sigma_j} v_j \right\|
          \leq \frac{\|r\|}{C_1}
\]
and a piece that may be large,
\[
  e_2 = \sum_{j={k+1}}^n \frac{\tilde{r}_j}{\sigma_j} v_j.
\]
Hence, backward stability implies that the error consists of a small
part and a part that lies in the ``nearly-singular subspace'' for the
matrix.
