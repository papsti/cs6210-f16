\section{Binary floating point}

Binary floating point arithmetic is essentially scientific notation.
Where in decimal scientific notation we write
\[
  \frac{1}{3}
  = 3.333\ldots \times 10^{-1},
\]
in floating point, we write
\[
  \frac{(1)_2}{(11)_2}
  = (1.010101\ldots)_2 \times 2^{-2}.
\]
Because computers are finite, however, we can only keep a finite
number of bits after the binary point.

\subsection{Normalized representations}

In general, a {\em normal floating point number} has the form
\[
  (-1)^s \times (1.b_1 b_2 \ldots b_p)_2 \times 2^{E},
\]
where $s \in \{0,1\}$ is the {\em sign bit},
$E$ is the {\em exponent}, and
$(1.b_2 \ldots b_p)_2$ is the {\em significand}.
In the 64-bit double precision format, $p = 52$ bits are used to store
the significand, 11 bits are used for the exponent, and one bit is
used for the sign.  The valid exponent range for normal floating point
numbers is $-1023 < E < 1024$; this leaves two exponent encodings left
over for special purpose.  One of these special exponents is used to
encode {\em subnormal numbers} of the form
\[
  (-1)^s \times (0.b_1 b_2 \ldots b_p)_2 \times 2^{-1022};
\]
the other special exponent is used to encode $\pm \infty$ and NaN
(Not a Number).




\section{Basic floating point arithmetic}

For a general real number $x$, we will write
\[
  \fl(x) = \mbox{ correctly rounded floating point representation of } x.
\]
By default, ``correctly rounded'' means that we find the closest
floating point number to $x$, breaking any ties by rounding to the
number with a zero in the last bit\footnote{%
There are other rounding modes beside the default, but we will not
discuss them in this class}.
If $x$ exceeds the largest normal floating point number,
then $\fl(x) = \infty$.

For basic operations (addition, subtraction, multiplication,
division, and square root), the floating point standard specifies that
the computer should produce the {\em true result, correctly rounded}.
So the MATLAB statement
\lstset{language=matlab,columns=flexible}
\begin{lstlisting}
    % Compute the sum of x and y (assuming they are exact)
    z = x + y;
\end{lstlisting}
actually computes the quantity $\hat{z} = \fl(x+y)$.  If $\hat{z}$ is
a normal double-precision floating point number, it will agree with
the true $z$ to 52 bits after the binary point.  That is, the relative
error will be smaller in magnitude than the {\em machine epsilon}
$\macheps = 2^{-53} \approx 1.1 \times 10^{-16}$:
\[
  \hat{z} = z(1+\delta), \quad |\delta| < \macheps.
\]
More generally, basic operations that produce normalized numbers are
correct to within a relative error of $\macheps$.  The floating
point standard also recommends that common transcendental functions,
such as exponential and trig functions, should be correctly rounded,
though compliant implementations that do not comply with this
recommendation may produce results with a relative error
just slightly larger than $\macheps$.

The fact that normal floating point results have a relative error
less than $\macheps$ gives us a useful {\em model} for reasoning about
floating point error.  We will refer to this as the ``$1 + \delta$''
model.  For example, suppose $x$ is an exactly-represented input to
the MATLAB statement
\begin{lstlisting}
    z = 1-x*x;
\end{lstlisting}
We can reason about the error in the computed $\hat{z}$ as follows:
\begin{align*}
  t_1 &= \fl(x^2) = x^2 (1+\delta_1) \\
  t_2 &= 1-t_1 = (1-x^2)\left( 1 + \frac{\delta_1 x^2}{1-x^2} \right) \\
  \hat{z}
  &= \fl(1-t_1)
    = z \left( 1 + \frac{\delta_1 x^2}{1-x^2} \right)(1+\delta_2) \\
  & \approx z \left( 1 + \frac{\delta_1 x^2}{1-x^2} +\delta_2 \right),
\end{align*}
where $|\delta_1|, |\delta_2| \leq \macheps$.  As before, we throw
away the (tiny) term involving $\delta_1 \delta_2$.
Note that if $z$ is close to zero (i.e.~if there is {\em cancellation} in the
subtraction), then the model shows the result may have a
large relative error.

\section*{Exceptions}

We say there is an {\em exception} when the floating point result is
not an ordinary value that represents the exact result.  The most
common exception is {\em inexact} (i.e. some rounding was needed).
Other exceptions occur when we fail to produce a normalized floating
point number.  These exceptions are:
\begin{description}
\item[Underflow:]
  An expression is too small to be represented as a normalized floating
  point value.  The default behavior is to return a subnormal.
\item[Overflow:]
  An expression is too large to be represented as a floating point
  number.  The default behavior is to return {\tt inf}.
\item[Invalid:]
  An expression evaluates to Not-a-Number (such as $0/0$)
\item[Divide by zero:]
  An expression evaluates ``exactly'' to an infinite value
  (such as $1/0$ or $\log(0)$).
\end{description}
When exceptions other than inexact occur, the usual ``$1 + \delta$''
model used for most rounding error analysis is not valid.
