\section{Lanczos and Arnoldi eigensolvers}

The standard ingredients in all the subspace methods we have
described so far are a choice of an approximation subspace
(usually a Krylov subspace) and a method for choosing an approximation
from the space.  In the most common methods for large-scale eigensolvers,
one uses a Krylov subspace together with a Bubnov-Galerkin condition
for choosing approximate eigenpairs; that is, we choose $v \in \mathcal{V}$
such that
\[
  r = (A-\hat{\lambda} I) \hat{v} \perp \mathcal{V}.
\]
In the symmetric case, this is equivalent to finding a constrained
stationary point of the Rayleigh quotient (i.e.~a $\hat{v}$ such that
directional derivatives of $\rho_A(\hat{v})$ are zero for any direction
in the space).  This approximation scheme is known as the {\em
Rayleigh-Ritz} method, and approximate eigenvectors and eigenvalues
obtained in this way are often called {\em Ritz vectors} and {\em Ritz
values}.

In the Lanczos method for the symmetric eigenvalue problem,
we compute the Lanczos decomposition
\[
  AQ_m = Q_m T_m + \beta_m q_{m+1} e_m^T,
\]
and use it to compute the residual relation for an approximate pair
$(\mu, Q_m y)$ by
\[
  r = (A-\mu I) Q_m y = Q_m (T_m-\mu I) y + \beta_m q_{m+1} e_m^T y.
\]
The condition $Q_m^T r = 0$ gives us the {\em projected} problem
\[
  (T_m-\mu I) y = 0;
\]
if we satisfy this condition, we have
\[
  r = \beta_m q_{m+1} y_m
\]
and the residual norm (in the 2-norm) is $|\beta_m y_m|$.  Generalizing,
if we compute the eigendecomposition
\[
  T_m = Y \Theta Y^T,
\]
we have the collected approximations $Z = Q_m Y_m$ with residuals
\[
  \|Az_k-z_k \theta_k\|_2 = |\beta_m| |e_m^T y_k|.
\]
This is useful because, as we discussed before, in the symmetric case
a small residual error implies a small distance to the closest eigenvalue.
This is also useful because the residual error can be computed with no
further matrix operations --- we need only to look at quantities that
we would already compute in the process of obtaining the tridiangonal
coefficients and the corresponding Ritz values.

The Arnoldi method for computing approximate eigenpairs similarly uses
the Galerkin condition together with the Arnoldi decomposition to
express an approximate partial Schur form.  From the decomposition
\[
  AQ_m = Q_m H_m + h_{m+1,m} q_{m+1} e_m^T,
\]
we write a subspace residual for $(Q_m Y, T)$ as
\[
  R = AQ_m Y - Q_m Y T = Q_m (H_m Y - Y T) + h_{m+1,m} q_{m+1} e_m^T.
\]
Forcing $Q_m^T R = 0$ gives the projected problem
\[
  H_m Y = Y T,
\]
i.e.~we seek a Schur decomposition of the (already Hessenberg)
matrix $H_m$.

There are three main issues with the Lanczos and Arnoldi methods
that we need to address in practical situations.
\begin{enumerate}
\item
  We must deal with forward instability, particularly in the case of the
  Lanczos method.  Unless we are careful to maintain orthogonality
  between the computed Lanczos basis vectors, the method derails.  The
  result is not that we get bad approximate eigenpairs; indeed, the
  forward instability is intimately tied to the very thing we want,
  which is convergence of eigenpairs.  The real problem is that we get
  the same eigenpairs over and over again, a phenomenon known as
  ``ghost'' eigenvalue approximations.  We deal with this issue by
  careful re-orthogonalization (selective or complete).
\item
  Because of the cost of storing a Krylov basis and maintaining its
  orthogonality, we typically only want to approximate a few eigenpairs
  at a time.
\item
  The Krylov subspace generated by $A$ and some random start
  vector contains iterates of the power method applied to any $A$
  (or to $A-\sigma I$ for any shift $\sigma$ --- the Krylov subspace
  is shift-invariant).  This is at least as good as power iteration
  for approximating the extremal parts of the spectrum, and we can
  use the same Chebyshev-based games we discussed before to
  give concrete (though typically pessimistic) convergence bounds.  But if
  eigenvalues cluster, or if we are interested in eigenvalues that
  are not at the edge of the spectrum, then the convergence in theory
  and in practice can be painfully slow.
\end{enumerate}
We address these issues with two basic techniques, both of which
we have already seen in other contexts: {\em spectral transformation}
and {\em restarting}.
