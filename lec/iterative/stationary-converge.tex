\section{Convergence of stationary iterations}

For general non-symmetric (and nonsingular) matrices, none of
the classical iterations is guaranteed to converge.
But there are a few classes of problems for which we can say something
about the convergence of the classical iterations, and we survey some
of these now.

\subsection{Strictly row diagonally-dominant problems}

Suppose $A$ is strictly diagonally dominant.  Then by definition,
the iteration matrix for Jacobi iteration ($R = D^{-1} (L+U)$)
must satisfy $\|R\|_\infty < 1$, and therefore Jacobi iteration
converges in this norm.  A bound on the rate of convergence has
to do with the strength of the diagonal dominance.  Moreover,
one can show (though we will not) that in this case
\[
  \|(D-L)^{-1} U\|_\infty \leq \|D^{-1}(L+U)\|_\infty < 1,
\]
so Gauss-Seidel converges at least as quickly as Jacobi.  The
Richardson iteration is also guaranteed to converge, at least so
long as $\omega < 1/(\max_i |a_ii|)$, since this is sufficient to
guarantee that all the Gershgorin disks of $I-\omega A$ will
remain within the unit circle.

\subsection{Symmetric and positive definite problems}

\subsubsection{Richardson iteration}

If $A$ is SPD with eigenvalues
$0 < \lambda_1 < \ldots < \lambda_n$, then Richardson
iteration satisfies
\[
  \|R\|_2 = \max(|1-\omega \lambda_1|, |1-\omega \lambda_n|);
\]
and the rate of convergence is optimal when
$\omega = 2/(\lambda_1 + \lambda_n)$,
which yields
\[
  \|R\|_2
    = 1 - \frac{2 \lambda_1}{\lambda_1 + \lambda_n}
    = 1 - \frac{2}{\kappa(A) + 1}
\]
If $A$ is ill-conditioned, the iteration may be painfully slow.

\subsubsection{Jacobi iteration}

The error iteration for Jacobi is
\[
  e^{(k+1)} = D^{-1} (L+U) e^{(k)} = D^{-1} (D-A) e^{(k)}.
\]
If $A$ is SPD, then so is $D$, and therefore it induces a norm;
scaling the error iteration by $D^{1/2}$ gives
\[
  \hat{e}^{(k+1)} = D^{-1/2} (D-A) D^{-1/2} \hat{e}^{(k)},
\]
where $\hat{e}^{(k)} = D^{1/2} e^{(k)}$ and
\[
  \|\hat{e}^{(k)}\|_2 = \|e^{(k)}\|_D.
\]
Therefore
\[
  \|e^{(k+1)}\|_D \leq \|D^{-1/2} (D-A) D^{-1/2}\|_2 \|e^{(k)}\|_D.
\]
For $A$ is symmetric and positive definite, we then have
\[
  \| D^{-1/2} (D-A) D^{-1/2} \|_2 = \max(|1-\lambda_1|, |1-\lambda_n|),
\]
where $0 < \lambda_1 < \ldots < \lambda_n$ are the eigenvalues of the
pencil $(A,D)$.  We have convergence when $\lambda_n < 2$,
i.e.~$2D-A$ is symmetric and positive definite.  Damped Jacobi, on
the other hand, can always be made to converge for a sufficiently
large damping level $\omega$.

The same analysis holds for block Jacobi.

\subsubsection{Gauss-Seidel iteration}

To understand the Gauss-Seidel convergence, it is useful to look at
the linear system $Ax^{(*)} = b$ as the minimizer of the convex quadratic
\[
  \phi(x) = \frac{1}{2} x^T A x - x^T b.
\]
Now consider a given $x$ and consider what happens if we update to
$x + s e_i$ for some $s$.  This gives the value
\[
  \phi(x+s e_i) = \phi(x) + \frac{a_{ii}}{2} s^2 + s e_i^T A x - s b_i.
\]
Minimizing with respect to $s$ yields
\[
  a_{ii} s = b_i - e_i^T A x
\]
or
\[
  a_{ii} (x_i + s) = b_i - \sum_{j \neq i} a_{ij} x_j.
\]
But this is precisely the Gauss-Seidel update!  Hence, Gauss-Seidel for
a positive definite system corresponds to optimization of $\phi$ by
{\em cyclic coordinate descent}.  The method decreases $\phi$ at each
coordinate step, and each sweep is guaranteed to sufficiently reduce
the objective so that we ultimately converge.

The same analysis holds for block Gauss-Seidel.

\subsection{Convergence on the 2D model problem}

In the case of the 2D model problem, recall that the eigenvalues are
\[
  \lambda_{i,j} =
  2\left(
    2-\cos(\pi i h)-\cos (\pi j h)
  \right)
\]
The extreme eigenvalues are
\[
  \lambda_{1,1} = 2 h^2 \pi^2 + O(h^4)
\]
and
\[
  \lambda_{n,n} = 4-2 h^2 \pi^2 + O(h^4).
\]
The diagonal of $T_{n\times n}$ is simply $4I$, so the Jacobi
iteration matrix looks like
\[
  R = \frac{1}{4} (4I-T_{n\times n}),
\]
or which the eigenvalues are
\[
  \lambda_{i,j}(R) = -(\cos(\pi i h)+\cos(\pi j h))/2,
\]
and the spectral radius is
\[
  \rho(R) = \cos(\pi h) = 1-\frac{\pi^2 h^2}{2} + O(h^4)
\]
Thus, the number of iterations to reduce the error by $1/e$ scales like
\[
  \frac{2}{\pi^2 h^2} = \frac{2}{\pi^2} (n+1)^2 = O(N);
\]
and since each step takes $O(N)$ time, the total time to reduce the
error by a constant factor scales like $O(N^2)$.

he successive overrelaxation iteration uses a splitting
\[
  M = \omega^{-1} (D - \omega \tilde{L}) = \omega^{-1} D^{-1} (I-\omega L),
\]
which yields an iteration matrix
\[
  R_{SOR} = (I-\omega L)^{-1} ( (1-\omega) I + \omega U).
\]
In general, this is rather awkward to deal with, since it is a
nonsymmetric matrix.  However, for the model problem with a particular
ordering of unknowns (red-black ordering), one has that the
eigenvalues $\mu$ of $R_{J}$ correspond to the eigenvalues $\lambda$
of $R_{SOR}$ via
\[
  (\lambda + \omega - 1)^2 = \lambda \omega^2 \mu^2.
\]
For the case $\omega = 1$ (Gauss-Seidel), this degenerates to
\[
  \lambda = \mu^2,
\]
and so $\rho(R_{GS}) = \rho(R_J)^2$.  Consequently, each Gauss-Seidel
iteration reduces the error by the same amount as two Jacobi
iterations, i.e.~Gauss-Seidel converges twice as fast on the model
problem.  This tends to be true for other problems similar to the
model problem, too.  However, going from Jacobi to Gauss-Seidel only
improves the convergence rate by a constant factor; it doesn't improve
the asymptotic complexity at all.  However optimal $\omega$ (about $2 -
O(h)$) gives us a spectral radius of $1-O(h)$ rather than $1-O(h^2)$,
allowing us to accelerate convergence to $O(N^{3/2})$.

The red-black ordering can be convenient for parallel implementation,
because allowing the red nodes (or black nodes) to be processed in any
orer gives more flexibility for different scheduling choices.  But it
is also a useful choice for analysis.  For example, in the red-black
ordering, the model problem looks like
\[
  A = \begin{bmatrix} 4I & B \\ B^T & 4I \end{bmatrix}
\]
The preconditioner based on Jacobi iteration is
\[
  M_J = \begin{bmatrix} 4I & 0 \\ 0 & 4 I \end{bmatrix},
\]
which results in the iteration matrix
\[
  R_J = M_J^{-1} (M_J-A) =
  \frac{1}{4} \begin{bmatrix} 0 & B \\ B^T & 0 \end{bmatrix}.
\]
The eigenvalues of $R_J$ are thus plus or minus one quarter the
singular values of $B$.  Note that this much would have been the same
for more general problems with the same structure!

I did not drag you in class through the rest of the analysis, and I
would not expect you to repeat it on an exam.  Nonetheless, it may be
worth writing it out in order to satisfy the curious.
The preconditioner for Gauss-Seidel is
\[
  M_{GS} = \begin{bmatrix} 4I & 0 \\ B^T & 4I \end{bmatrix};
\]
and because of the relatively simple form of this matrix, we have
\[
  M_{GS}^{-1} =
  \frac{1}{4}
  \begin{bmatrix}
    I & 0 \\
    B^T/4 & I
  \end{bmatrix}.
\]
The iteration matrix for Gauss-Seidel is
\[
  R_{GS} = M_{GS}^{-1} (M_{GS}-A) =
  \begin{bmatrix} 0 & B/4 \\ 0 & -\frac{1}{16} B^T B \end{bmatrix},
\]
which has several zero eigenvalues together with some eigenvalues that
are minus $1/16$ times the squared singular values of $B^T B$.  Thus,
as indicated earlier, the spectral radius of $R_{GS}$ is the square of
the spectral radius of $R_{J}$ (for the model problem).

The analysis for the general SOR case is slightly messier, but I'll
include it here for completeness.  The preconditioner is
\[
  M_{SOR} = \frac{1}{\omega}
  \begin{bmatrix} 4I & 0 \\ \omega B^T & 4I \end{bmatrix},
\]
and the inverse is
\[
  M_{SOR}^{-1} =
  \frac{\omega}{4} \begin{bmatrix} I & 0 \\ -\omega B^T/4 & I \end{bmatrix},
\]
The iteration matrix is
\begin{align*}
  R_{SOR} &=
  \frac{1}{4} \begin{bmatrix} I & 0 \\ -\omega B^T/4 & I \end{bmatrix}
  \begin{bmatrix} 4(1-\omega) I & -\omega B \\ 0 & 4(1-\omega) I \end{bmatrix} \\
  &=
  \begin{bmatrix}
    (1-\omega) I & -\omega B/4 \\
    -(1-\omega) \omega B^T/4 & \omega^2 B^T B/16 + (1-\omega) I
  \end{bmatrix}.
\end{align*}
If $\lambda$ is any eigenvalue of $R_{SOR}$ except $1-\omega$, we can do
partial Gaussian elimination on the eigenvalue equation
\[
  (R_{SOR}-\mu I) v = 0;
\]
after eliminating the first block of variables, we have the residual system
\[
  \left(\frac{\omega^2}{16} B^T B-(\lambda+\omega-1) I - \frac{(1-\omega)\omega^2}{16} B^T ((1-\omega-\lambda) I)^{-1} B\right)
v_2 = 0,
\]
Refactoring, we have
\[
  \left[
\left( \frac{1-\omega}{\lambda+\omega-1} + 1 \right)
  \frac{\omega^2}{16} B^T B - (\lambda+\omega-1) I \right] v_2 = 0.
\]
From our earlier arguments, letting $\mu$ be an eigenvalue of the
Jacobi matrix, we know that $\mu^2$ is an eigenvalue of $B^T B/16$.  The
corresponding eigenvalues $\lambda$ of $R_{SOR}$ must therefore
satisfy
\[
  \left( \frac{1-\omega}{\lambda+\omega-1} + 1 \right) \omega^2 \mu^2 - (\lambda-\omega-1) = 0.
\]
Multiplying through by $\lambda-\omega-1$, we have
\[
  (1-\omega + \lambda + \omega - 1) \omega^2 \mu^2 -
  (\lambda-\omega-1)^2 = 0
\]
or
\[
 \lambda \omega^2 \mu^2 = (\lambda - \omega - 1)^2,
\]
which is the formula noted before.
