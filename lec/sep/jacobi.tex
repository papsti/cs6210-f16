\section{Jacobi iteration}

A {\em Jacobi rotation} is a symmetric transformation that
diagonalizes a $2 \times 2$ (sub)matrix:
\[
  J^T A J = \Lambda
\]
In terms of scalars, this means solving
\[
  \begin{bmatrix} c & -s \\ s & c \end{bmatrix}
  \begin{bmatrix} \alpha & \beta \\ \beta & \gamma \end{bmatrix}
  \begin{bmatrix} c & s \\ -s & c \end{bmatrix} =
  \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix},
\]
where $c = \cos(\theta)$ and $s = \sin(\theta)$.  A numerically
stable method for carrying out this computation is described in
Golub and Van Loan\footnote{Or on Wikipedia!}; we will leave
the details aside for the purposes of these notes.

Given an algorithm for computing Jacobi rotations, the idea
of the Jacobi iteration is to incrementally apply Jacobi rotations
to each $2 \times 2$ principle minors of $A$ in turn.  The code is
remarkably simple:

\lstinputlisting{code/eigen/jacobi_sweep.m}

Each time we apply a Jacobi iteration to rows and columns $k$ and $l$,
we reduce the sum of squares off the main diagonal by $a_{kl}^2$. The
iteration converges quadratically, and typically we can stop after 5--10
sweeps.  Each sweep costs $O(n^3)$, and has cost comparable to the cost
of a tridiagonalization step before running QR iteration.  Hence, Jacobi
iteration is rather slow.  On the other hand, it tends to compute the
small eigenvalues of $A$ much more accurate than competing methods that
start with a tridiagonalization step.
